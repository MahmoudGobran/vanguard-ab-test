{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aac68dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.stats.proportion import proportions_ztest, confint_proportions_2indep\n",
    "\n",
    "# Set display options for better viewing\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Define data path using raw string or double backslashes\n",
    "data_path = r'C:\\Users\\Mahmoud Gobran\\00_Ironhack\\Projects\\vanguard-ab-test\\data\\raw'\n",
    "\n",
    "# File names\n",
    "files = {\n",
    "    'demo': 'df_final_demo.csv',\n",
    "    'web_pt_1': 'df_final_web_data_pt_1.csv',\n",
    "    'web_pt_2': 'df_final_web_data_pt_2.csv',\n",
    "    'experiment': 'df_final_experiment_clients.csv'\n",
    "}\n",
    "\n",
    "# Try loading the datasets\n",
    "try:\n",
    "    df_final_demo = pd.read_csv(os.path.join(data_path, files['demo']))\n",
    "    df_final_web_data_pt_1 = pd.read_csv(os.path.join(data_path, files['web_pt_1']))\n",
    "    df_final_web_data_pt_2 = pd.read_csv(os.path.join(data_path, files['web_pt_2']))\n",
    "    df_final_experiment_clients = pd.read_csv(os.path.join(data_path, files['experiment']))\n",
    "    print(\"All datasets loaded successfully!\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Merge web data\n",
    "#df_final_web_data = pd.concat([df_final_web_data_pt_1, df_final_web_data_pt_2], ignore_index=True)\n",
    "#print(\"\\nWeb data parts merged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fea276e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_data_report(df, file_path=\"df_name.txt\", max_unique=50):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        def p(*args): print(*args, file=f)\n",
    "\n",
    "        p(\"=== Shape (rows, columns) ===\")\n",
    "        p(df.shape)\n",
    "\n",
    "        p(\"\\n=== Column Names ===\")\n",
    "        p(df.columns.tolist())\n",
    "\n",
    "        p(\"\\n=== Data Types ===\")\n",
    "        p(df.dtypes)\n",
    "\n",
    "        p(\"\\n=== Missing Values (per column) ===\")\n",
    "        p(df.isnull().sum())\n",
    "\n",
    "        p(\"\\n=== Duplicate Rows ===\")\n",
    "        p(df.duplicated().sum())\n",
    "\n",
    "        p(\"\\n=== Unique Values (per column) ===\")\n",
    "        p(df.nunique())\n",
    "\n",
    "        p(\"\\n=== DataFrame Info ===\")\n",
    "        df.info(buf=f)\n",
    "\n",
    "        p(\"\\n=== Descriptive Statistics (including non-numeric) ===\")\n",
    "        p(df.describe(include='all'))\n",
    "\n",
    "        p(\"\\n=== First 5 Rows ===\")\n",
    "        p(df.head())\n",
    "\n",
    "        p(\"\\n=== Last 5 Rows ===\")\n",
    "        p(df.tail())\n",
    "\n",
    "        p(\"\\n=== Random Sample of 5 Rows ===\")\n",
    "        p(df.sample(5))\n",
    "\n",
    "        p(\"\\n=== Value Counts (for columns with 50 or fewer unique values) ===\")\n",
    "        for col in df.columns:\n",
    "            if df[col].nunique() <= max_unique:\n",
    "                p(f\"\\n--- {col} ---\")\n",
    "                p(df[col].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0a656fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_data_report(df_final_demo, file_path=\"report_df_final_demo.txt\")\n",
    "\n",
    "quick_data_report(df_final_web_data_pt_1, file_path=\"report_df_final_web_data_pt_1.txt\")\n",
    "\n",
    "quick_data_report(df_final_web_data_pt_2, file_path=\"report_df_final_web_data_pt_2.txt\")\n",
    "\n",
    "quick_data_report(df_final_experiment_clients, file_path=\"report_df_final_experiment_clients.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db64d6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Cleaning and Initial EDA ---\n",
      "\n",
      "#### 1. Concatenating df_final_web_data_pt_1 and df_final_web_data_pt_2 ####\n",
      "Initial rows in df_final_web_data_pt_1: 343141\n",
      "Initial rows in df_final_web_data_pt_2: 412264\n",
      "Combined df_final_web_data now has 755405 entries.\n",
      "Proof (head of combined web data):\n",
      "   client_id            visitor_id                      visit_id process_step        date_time\n",
      "0    9988021  580560515_7732621733  781255054_21935453173_531117       step_3  4/17/2017 15:27\n",
      "1    9988021  580560515_7732621733  781255054_21935453173_531117       step_2  4/17/2017 15:26\n",
      "2    9988021  580560515_7732621733  781255054_21935453173_531117       step_3  4/17/2017 15:19\n",
      "3    9988021  580560515_7732621733  781255054_21935453173_531117       step_2  4/17/2017 15:19\n",
      "4    9988021  580560515_7732621733  781255054_21935453173_531117       step_3  4/17/2017 15:18\n",
      "\n",
      "Proof (info of combined web data):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 755405 entries, 0 to 755404\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   client_id     755405 non-null  int64 \n",
      " 1   visitor_id    755405 non-null  object\n",
      " 2   visit_id      755405 non-null  object\n",
      " 3   process_step  755405 non-null  object\n",
      " 4   date_time     755405 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 28.8+ MB\n",
      "None\n",
      "\n",
      "#### 2. Converting 'date_time' to datetime objects and sorting web data ####\n",
      "Original 'date_time' Dtype: object\n",
      "New 'date_time' Dtype: datetime64[ns]\n",
      "Sorting df_final_web_data by client_id, visit_id, and date_time...\n",
      "Web data sorted successfully.\n",
      "\n",
      "Proof (head of sorted web data, showing date_time type and order):\n",
      "   client_id             visitor_id                      visit_id process_step           date_time\n",
      "0        169  201385055_71273495308  749567106_99161211863_557568       step_1 2017-04-12 20:19:00\n",
      "1        169  201385055_71273495308  749567106_99161211863_557568        start 2017-04-12 20:19:00\n",
      "2        169  201385055_71273495308  749567106_99161211863_557568       step_2 2017-04-12 20:20:00\n",
      "3        169  201385055_71273495308  749567106_99161211863_557568       step_3 2017-04-12 20:22:00\n",
      "4        169  201385055_71273495308  749567106_99161211863_557568      confirm 2017-04-12 20:23:00\n",
      "\n",
      "#### 3. Handling missing values in df_final_demo ####\n",
      "\n",
      "Step 3.1: Identifying and Quantifying Missing Values (before cleaning)\n",
      "Counts of missing values per column:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Percentage of missing values per column:\n",
      "Series([], dtype: float64)\n",
      "\n",
      "Total rows in df_final_demo before dropping NaNs: 70594\n",
      "Number of rows dropped: 0\n",
      "\n",
      "Step 3.3: Proof of Resolution (after cleaning)\n",
      "Missing values in df_final_demo (after cleaning):\n",
      "client_id           0\n",
      "clnt_tenure_yr      0\n",
      "clnt_tenure_mnth    0\n",
      "clnt_age            0\n",
      "gendr               0\n",
      "num_accts           0\n",
      "bal                 0\n",
      "calls_6_mnth        0\n",
      "logons_6_mnth       0\n",
      "dtype: int64\n",
      "Shape of df_final_demo after cleaning: (70594, 9)\n",
      "\n",
      "#### 4. Cleaning 'gendr' column in df_final_demo ####\n",
      "Unique genders before cleaning: ['Unknown' 'Male' 'Female' 'X']\n",
      "Unique genders after cleaning: ['Unknown' 'Male' 'Female' 'X']\n",
      "Proof (value counts after cleaning):\n",
      "gendr\n",
      "Unknown    24122\n",
      "Male       23724\n",
      "Female     22745\n",
      "X              3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "#### 5. Checking for and removing duplicates across dataframes ####\n",
      "Checking df_final_demo for full row duplicates...\n",
      "  No duplicates found in df_final_demo.\n",
      "Checking df_final_web_data for duplicates based on ['client_id', 'visitor_id', 'visit_id', 'process_step', 'date_time']...\n",
      "  Removed 43534 duplicates from df_final_web_data.\n",
      "Checking df_final_experiment_clients for full row duplicates...\n",
      "  No duplicates found in df_final_experiment_clients.\n",
      "\n",
      "Proof (Final shapes after duplicate removal):\n",
      "df_final_demo shape: (70594, 9)\n",
      "df_final_web_data shape: (711871, 5)\n",
      "df_final_experiment_clients shape: (70609, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.stats.proportion import proportions_ztest, confint_proportions_2indep\n",
    "\n",
    "print(\"--- Starting Data Cleaning and Initial EDA ---\")\n",
    "\n",
    "### --- Data Cleaning Steps ---\n",
    "\n",
    "#### **1. Concatenate Web Data Parts** ####\n",
    "# Requirement: Combine the two parts of the web interaction data into a single DataFrame.\n",
    "# Goal: Create a unified dataset for all web events.\n",
    "print(\"\\n#### 1. Concatenating df_final_web_data_pt_1 and df_final_web_data_pt_2 ####\")\n",
    "print(\"Initial rows in df_final_web_data_pt_1:\", len(df_final_web_data_pt_1))\n",
    "print(\"Initial rows in df_final_web_data_pt_2:\", len(df_final_web_data_pt_2))\n",
    "\n",
    "df_final_web_data = pd.concat([df_final_web_data_pt_1, df_final_web_data_pt_2], ignore_index=True)\n",
    "\n",
    "print(f\"Combined df_final_web_data now has {len(df_final_web_data)} entries.\")\n",
    "print(\"Proof (head of combined web data):\")\n",
    "print(df_final_web_data.head())\n",
    "print(\"\\nProof (info of combined web data):\")\n",
    "print(df_final_web_data.info())\n",
    "\n",
    "\n",
    "#### **2. Convert 'date_time' to Datetime Objects and Sort Web Data** ####\n",
    "# Requirement: Convert the 'date_time' column from object (string) to datetime objects for time-based calculations.\n",
    "# Requirement: Sort the web data by client_id, visit_id, and date_time to ensure correct sequential processing of steps.\n",
    "# Goal: Enable accurate calculation of time spent and detection of backward movements.\n",
    "print(\"\\n#### 2. Converting 'date_time' to datetime objects and sorting web data ####\")\n",
    "\n",
    "print(\"Original 'date_time' Dtype:\", df_final_web_data['date_time'].dtype)\n",
    "df_final_web_data['date_time'] = pd.to_datetime(df_final_web_data['date_time'])\n",
    "print(\"New 'date_time' Dtype:\", df_final_web_data['date_time'].dtype)\n",
    "\n",
    "print(\"Sorting df_final_web_data by client_id, visit_id, and date_time...\")\n",
    "df_final_web_data = df_final_web_data.sort_values(by=['client_id', 'visit_id', 'date_time']).reset_index(drop=True)\n",
    "print(\"Web data sorted successfully.\")\n",
    "\n",
    "print(\"\\nProof (head of sorted web data, showing date_time type and order):\")\n",
    "print(df_final_web_data.head())\n",
    "\n",
    "\n",
    "#### **3. Handle Missing Values in `df_final_demo`** ####\n",
    "# Requirement: Identify, quantify, and address missing values within the `df_final_demo` DataFrame.\n",
    "# Goal: Ensure data integrity for demographic analysis by removing incomplete client records.\n",
    "\n",
    "print(\"\\n#### 3. Handling missing values in df_final_demo ####\")\n",
    "\n",
    "# Step 3.1: Identify and Quantify Missing Values\n",
    "print(\"\\nStep 3.1: Identifying and Quantifying Missing Values (before cleaning)\")\n",
    "missing_values_count = df_final_demo.isnull().sum()\n",
    "print(\"Counts of missing values per column:\")\n",
    "print(missing_values_count[missing_values_count > 0])\n",
    "\n",
    "missing_values_percentage = (df_final_demo.isnull().sum() / len(df_final_demo)) * 100\n",
    "print(\"\\nPercentage of missing values per column:\")\n",
    "print(missing_values_percentage[missing_values_percentage > 0])\n",
    "\n",
    "print(f\"\\nTotal rows in df_final_demo before dropping NaNs: {len(df_final_demo)}\")\n",
    "\n",
    "# Step 3.2: Choose and Implement Strategy (Dropping Rows)\n",
    "# Observation: Given the very small percentage of missing values (<0.03%) and that they are concentrated\n",
    "# across multiple columns for the same few rows, dropping these rows is the most appropriate strategy.\n",
    "initial_demo_rows = len(df_final_demo)\n",
    "df_final_demo.dropna(inplace=True)\n",
    "\n",
    "print(f\"Number of rows dropped: {initial_demo_rows - len(df_final_demo)}\")\n",
    "\n",
    "\n",
    "# Step 3.3: Proof of Resolution\n",
    "print(\"\\nStep 3.3: Proof of Resolution (after cleaning)\")\n",
    "print(\"Missing values in df_final_demo (after cleaning):\")\n",
    "print(df_final_demo.isnull().sum())\n",
    "print(f\"Shape of df_final_demo after cleaning: {df_final_demo.shape}\")\n",
    "\n",
    "\n",
    "#### **4. Clean 'gendr' Column in `df_final_demo`** ####\n",
    "# Requirement: Standardize the 'gendr' column to consistent, human-readable labels.\n",
    "# Goal: Improve data quality and readability for demographic analysis.\n",
    "\n",
    "print(\"\\n#### 4. Cleaning 'gendr' column in df_final_demo ####\")\n",
    "\n",
    "print(\"Unique genders before cleaning:\", df_final_demo['gendr'].unique())\n",
    "\n",
    "# change 'M', 'F', 'U' if present.\n",
    "df_final_demo['gendr'] = df_final_demo['gendr'].replace({'M': 'Male', 'F': 'Female', 'U': 'Unknown'})\n",
    "\n",
    "print(\"Unique genders after cleaning:\", df_final_demo['gendr'].unique())\n",
    "print(\"Proof (value counts after cleaning):\")\n",
    "print(df_final_demo['gendr'].value_counts())\n",
    "\n",
    "\n",
    "#### **5. Check for and Remove Duplicates Across All DataFrames** ####\n",
    "# Requirement: Ensure no identical rows exist within each DataFrame that could skew analysis.\n",
    "# Goal: Maintain data accuracy and prevent double-counting of events or clients.\n",
    "\n",
    "print(\"\\n#### 5. Checking for and removing duplicates across dataframes ####\")\n",
    "\n",
    "# List of dataframes to check for general row-level duplicates\n",
    "dataframes_to_check = {\n",
    "    'df_final_demo': df_final_demo,\n",
    "    'df_final_web_data': df_final_web_data,\n",
    "    'df_final_experiment_clients': df_final_experiment_clients\n",
    "}\n",
    "\n",
    "for df_name, df_obj in dataframes_to_check.items():\n",
    "    initial_rows = len(df_obj)\n",
    "    # For web data, define a specific subset to identify unique events (client, visitor, visit, step, time)\n",
    "    if df_name == 'df_final_web_data':\n",
    "        subset_cols = ['client_id', 'visitor_id', 'visit_id', 'process_step', 'date_time']\n",
    "        df_obj.drop_duplicates(subset=subset_cols, inplace=True)\n",
    "        print(f\"Checking {df_name} for duplicates based on {subset_cols}...\")\n",
    "    else:\n",
    "        df_obj.drop_duplicates(inplace=True)\n",
    "        print(f\"Checking {df_name} for full row duplicates...\")\n",
    "\n",
    "    if initial_rows - len(df_obj) > 0:\n",
    "        print(f\"  Removed {initial_rows - len(df_obj)} duplicates from {df_name}.\")\n",
    "    else:\n",
    "        print(f\"  No duplicates found in {df_name}.\")\n",
    "\n",
    "    # Update the dataframe in the dictionary for consistency\n",
    "    dataframes_to_check[df_name] = df_obj\n",
    "\n",
    "# Re-assign to original variables (if they were not updated in-place, which drop_duplicates does)\n",
    "df_final_demo = dataframes_to_check['df_final_demo']\n",
    "df_final_web_data = dataframes_to_check['df_final_web_data']\n",
    "df_final_experiment_clients = dataframes_to_check['df_final_experiment_clients']\n",
    "\n",
    "print(\"\\nProof (Final shapes after duplicate removal):\")\n",
    "print(f\"df_final_demo shape: {df_final_demo.shape}\")\n",
    "print(f\"df_final_web_data shape: {df_final_web_data.shape}\")\n",
    "print(f\"df_final_experiment_clients shape: {df_final_experiment_clients.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fbda5290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Merging Datasets ---\n",
      "\n",
      "Step 1: Merging df_final_experiment_clients with df_final_demo on 'client_id'...\n",
      "df_merged_clients has 70609 entries.\n",
      "Proof (head of df_merged_clients):\n",
      "   client_id variation  clnt_tenure_yr  clnt_tenure_mnth  clnt_age    gendr  num_accts        bal  calls_6_mnth  logons_6_mnth\n",
      "0    9988021      Test             5.0              64.0      79.0  Unknown        2.0  189023.86           1.0            4.0\n",
      "1    8320017      Test            22.0             274.0      34.5     Male        2.0   36001.90           5.0            8.0\n",
      "2    4033851   Control            12.0             149.0      63.5     Male        2.0  142642.26           5.0            8.0\n",
      "3    1982004      Test             6.0              80.0      44.5  Unknown        2.0   30231.76           1.0            4.0\n",
      "4    9294070   Control             5.0              70.0      29.0  Unknown        2.0   34254.54           0.0            3.0\n",
      "\n",
      "Proof (Columns in df_merged_clients): ['client_id', 'variation', 'clnt_tenure_yr', 'clnt_tenure_mnth', 'clnt_age', 'gendr', 'num_accts', 'bal', 'calls_6_mnth', 'logons_6_mnth']\n",
      "\n",
      "Proof (Null values in df_merged_clients after first merge):\n",
      "client_id               0\n",
      "variation           20109\n",
      "clnt_tenure_yr         15\n",
      "clnt_tenure_mnth       15\n",
      "clnt_age               15\n",
      "gendr                  15\n",
      "num_accts              15\n",
      "bal                    15\n",
      "calls_6_mnth           15\n",
      "logons_6_mnth          15\n",
      "dtype: int64\n",
      "\n",
      "Step 2: Merging df_final_web_data with df_merged_clients on 'client_id'...\n",
      "df_full has 711871 entries.\n",
      "Proof (head of df_full):\n",
      "   client_id             visitor_id                      visit_id process_step           date_time variation  clnt_tenure_yr  clnt_tenure_mnth  clnt_age gendr  num_accts        bal  calls_6_mnth  logons_6_mnth\n",
      "0        169  201385055_71273495308  749567106_99161211863_557568       step_1 2017-04-12 20:19:00       NaN            21.0             262.0      47.5  Male        2.0  501570.72           4.0            4.0\n",
      "1        169  201385055_71273495308  749567106_99161211863_557568        start 2017-04-12 20:19:00       NaN            21.0             262.0      47.5  Male        2.0  501570.72           4.0            4.0\n",
      "2        169  201385055_71273495308  749567106_99161211863_557568       step_2 2017-04-12 20:20:00       NaN            21.0             262.0      47.5  Male        2.0  501570.72           4.0            4.0\n",
      "3        169  201385055_71273495308  749567106_99161211863_557568       step_3 2017-04-12 20:22:00       NaN            21.0             262.0      47.5  Male        2.0  501570.72           4.0            4.0\n",
      "4        169  201385055_71273495308  749567106_99161211863_557568      confirm 2017-04-12 20:23:00       NaN            21.0             262.0      47.5  Male        2.0  501570.72           4.0            4.0\n",
      "\n",
      "Proof (Columns in df_full): ['client_id', 'visitor_id', 'visit_id', 'process_step', 'date_time', 'variation', 'clnt_tenure_yr', 'clnt_tenure_mnth', 'clnt_age', 'gendr', 'num_accts', 'bal', 'calls_6_mnth', 'logons_6_mnth']\n",
      "\n",
      "Proof (Null values in df_full after second merge):\n",
      "client_id                0\n",
      "visitor_id               0\n",
      "visit_id                 0\n",
      "process_step             0\n",
      "date_time                0\n",
      "variation           409474\n",
      "clnt_tenure_yr      288653\n",
      "clnt_tenure_mnth    288653\n",
      "clnt_age            288653\n",
      "gendr               288653\n",
      "num_accts           288653\n",
      "bal                 288653\n",
      "calls_6_mnth        288653\n",
      "logons_6_mnth       288653\n",
      "dtype: int64\n",
      "\n",
      "Step 3: Filtering df_full to include only 'Test' or 'Control' variations (df_full_ab_test)...\n",
      "Removed 409474 rows where 'variation' was NaN (clients not in experiment).\n",
      "df_full_ab_test now has 302397 entries.\n",
      "\n",
      "Proof (Value counts for 'variation' in df_full_ab_test):\n",
      "variation\n",
      "Test       169141\n",
      "Control    133256\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proof (Null values in df_full_ab_test after final filtering):\n",
      "client_id             0\n",
      "visitor_id            0\n",
      "visit_id              0\n",
      "process_step          0\n",
      "date_time             0\n",
      "variation             0\n",
      "clnt_tenure_yr      108\n",
      "clnt_tenure_mnth    108\n",
      "clnt_age            108\n",
      "gendr               108\n",
      "num_accts           108\n",
      "bal                 108\n",
      "calls_6_mnth        108\n",
      "logons_6_mnth       108\n",
      "dtype: int64\n",
      "\n",
      "--- Data Cleaning and Merging Complete ---\n"
     ]
    }
   ],
   "source": [
    "### --- Merging Datasets ---\n",
    "# Requirement: Combine the three distinct datasets (`df_final_demo`, `df_final_web_data`, `df_final_experiment_clients`)\n",
    "#              into a single, comprehensive DataFrame for unified analysis.\n",
    "# Goal: Create `df_full_ab_test` which contains all relevant data points (demographics, web events, experiment group)\n",
    "#       for clients actively participating in the A/B test.\n",
    "\n",
    "print(\"\\n--- Merging Datasets ---\")\n",
    "\n",
    "# Ensure the 'Variation' column from df_final_experiment_clients is lowercase 'variation'\n",
    "# to match common Python/Pandas naming conventions and prevent KeyErrors.\n",
    "if 'Variation' in df_final_experiment_clients.columns:\n",
    "    df_final_experiment_clients.rename(columns={'Variation': 'variation'}, inplace=True)\n",
    "    print(\"Renamed 'Variation' column to 'variation' in df_final_experiment_clients.\")\n",
    "elif 'variation' not in df_final_experiment_clients.columns:\n",
    "    print(\"WARNING: 'variation' column not found in df_final_experiment_clients. Please check original data.\")\n",
    "\n",
    "\n",
    "# Step 1: Merge experiment clients with demographics\n",
    "# Use 'left' merge to keep all clients from the experiment assignment, enriching with their demographic data.\n",
    "print(\"\\nStep 1: Merging df_final_experiment_clients with df_final_demo on 'client_id'...\")\n",
    "df_merged_clients = pd.merge(df_final_experiment_clients, df_final_demo, on='client_id', how='left')\n",
    "print(f\"df_merged_clients has {len(df_merged_clients)} entries.\")\n",
    "print(\"Proof (head of df_merged_clients):\")\n",
    "print(df_merged_clients.head())\n",
    "print(\"\\nProof (Columns in df_merged_clients):\", df_merged_clients.columns.tolist()) # Check columns after merge 1\n",
    "print(\"\\nProof (Null values in df_merged_clients after first merge):\")\n",
    "print(df_merged_clients.isnull().sum())\n",
    "\n",
    "\n",
    "# Step 2: Merge the combined client data with the full web data\n",
    "# Use 'left' merge to keep all web interactions, enriching them with client demographics and experiment group.\n",
    "print(\"\\nStep 2: Merging df_final_web_data with df_merged_clients on 'client_id'...\")\n",
    "df_full = pd.merge(df_final_web_data, df_merged_clients, on='client_id', how='left')\n",
    "print(f\"df_full has {len(df_full)} entries.\")\n",
    "print(\"Proof (head of df_full):\")\n",
    "print(df_full.head())\n",
    "print(\"\\nProof (Columns in df_full):\", df_full.columns.tolist()) # Check columns after merge 2\n",
    "print(\"\\nProof (Null values in df_full after second merge):\")\n",
    "print(df_full.isnull().sum())\n",
    "\n",
    "\n",
    "# Step 3: Filter for A/B Test Participants Only\n",
    "# Requirement: For the A/B test analysis, focus only on clients explicitly assigned to 'Test' or 'Control' groups.\n",
    "# Goal: Create `df_full_ab_test` which is the final cleaned and merged dataset for primary analysis.\n",
    "\n",
    "print(\"\\nStep 3: Filtering df_full to include only 'Test' or 'Control' variations (df_full_ab_test)...\")\n",
    "initial_full_rows = len(df_full)\n",
    "df_full_ab_test = df_full.dropna(subset=['variation']).copy() # Filter out rows where 'variation' is NaN\n",
    "print(f\"Removed {initial_full_rows - len(df_full_ab_test)} rows where 'variation' was NaN (clients not in experiment).\")\n",
    "print(f\"df_full_ab_test now has {len(df_full_ab_test)} entries.\")\n",
    "print(\"\\nProof (Value counts for 'variation' in df_full_ab_test):\")\n",
    "print(df_full_ab_test['variation'].value_counts())\n",
    "print(\"\\nProof (Null values in df_full_ab_test after final filtering):\")\n",
    "print(df_full_ab_test.isnull().sum())\n",
    "\n",
    "print(\"\\n--- Data Cleaning and Merging Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f01a1b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved df_full_ab_test to: C:\\Users\\Mahmoud Gobran\\00_Ironhack\\Projects\\vanguard-ab-test\\data\\processed\\df_full_ab_test_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Save the Cleaned and Merged DataFrame for Next Steps ---\n",
    "# Define a path for processed data\n",
    "processed_data_path = r'C:\\Users\\Mahmoud Gobran\\00_Ironhack\\Projects\\vanguard-ab-test\\data\\processed'\n",
    "os.makedirs(processed_data_path, exist_ok=True) # Ensure the directory exists\n",
    "\n",
    "output_file_name = 'df_full_ab_test_cleaned.csv'\n",
    "output_full_path = os.path.join(processed_data_path, output_file_name)\n",
    "\n",
    "try:\n",
    "    df_full_ab_test.to_csv(output_full_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved df_full_ab_test to: {output_full_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving df_full_ab_test: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4067e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Now, the Initial EDA ---\n",
    "# EDA runs on the prepared df_full_ab_test.\n",
    "\n",
    "\n",
    "print(\"\\n--- Client Behavior Analysis (EDA) on Cleaned & Merged Data (df_full_ab_test) ---\")\n",
    "\n",
    "# Ensure we're using unique client_ids for demographic analysis to avoid bias from multiple web events per client.\n",
    "unique_clients_df = df_full_ab_test.drop_duplicates(subset=['client_id']).copy()\n",
    "\n",
    "# --- FIX FOR VALUEERROR: NaN in X-axis Ticks ---\n",
    "# Check for NaNs in 'logons_6_mnth' and 'calls_6_mnth'\n",
    "print(\"\\nChecking and cleaning 'logons_6_mnth' and 'calls_6_mnth' for plotting...\")\n",
    "print(\"NaNs in 'logons_6_mnth' before cleaning:\", unique_clients_df['logons_6_mnth'].isnull().sum())\n",
    "print(\"NaNs in 'calls_6_mnth' before cleaning:\", unique_clients_df['calls_6_mnth'].isnull().sum())\n",
    "\n",
    "# Fill NaNs with 0 (since missing logons/calls means 0 for a client) and convert to integer\n",
    "unique_clients_df['logons_6_mnth'] = unique_clients_df['logons_6_mnth'].fillna(0).astype(int)\n",
    "unique_clients_df['calls_6_mnth'] = unique_clients_df['calls_6_mnth'].fillna(0).astype(int)\n",
    "\n",
    "print(\"NaNs in 'logons_6_mnth' after cleaning:\", unique_clients_df['logons_6_mnth'].isnull().sum())\n",
    "print(\"NaNs in 'calls_6_mnth' after cleaning:\", unique_clients_df['calls_6_mnth'].isnull().sum())\n",
    "\n",
    "# Check if `num_accts` might have NaNs\n",
    "print(\"NaNs in 'num_accts' before cleaning:\", unique_clients_df['num_accts'].isnull().sum())\n",
    "unique_clients_df['num_accts'] = unique_clients_df['num_accts'].fillna(0).astype(int)\n",
    "print(\"NaNs in 'num_accts' after cleaning:\", unique_clients_df['num_accts'].isnull().sum())\n",
    "# --------------------------------------------------\n",
    "\n",
    "# 1. Who are the primary clients using this online process?\n",
    "# Overall demographics of clients present in the dataset (those who used the online process AND were in experiment)\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.histplot(unique_clients_df['clnt_age'], bins=20, kde=True, color='skyblue')\n",
    "plt.title('Client Age Distribution (Experiment Participants)', fontsize=14)\n",
    "plt.xlabel('Age', fontsize=12)\n",
    "plt.ylabel('Number of Unique Clients', fontsize=12)\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "# FutureWarning: Passing `palette` without assigning `hue` is deprecated...\n",
    "# To fix the warning: sns.countplot(x='gendr', data=unique_clients_df, palette='viridis', hue='gendr', legend=False)\n",
    "sns.countplot(x='gendr', data=unique_clients_df, palette='viridis')\n",
    "plt.title('Client Gender Distribution (Experiment Participants)', fontsize=14)\n",
    "plt.xlabel('Gender', fontsize=12)\n",
    "plt.ylabel('Number of Unique Clients', fontsize=12)\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.histplot(unique_clients_df['clnt_tenure_yr'], bins=10, kde=True, color='lightcoral')\n",
    "plt.title('Client Tenure (Years) Distribution (Experiment Participants)', fontsize=14)\n",
    "plt.xlabel('Years with Vanguard', fontsize=12)\n",
    "plt.ylabel('Number of Unique Clients', fontsize=12)\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "# Ensure bins are correctly set for integer counts\n",
    "sns.histplot(unique_clients_df['num_accts'], bins=np.arange(unique_clients_df['num_accts'].min(), unique_clients_df['num_accts'].max() + 2) - 0.5, stat='count', color='lightgreen')\n",
    "plt.title('Number of Accounts Distribution (Experiment Participants)', fontsize=14)\n",
    "plt.xlabel('Number of Accounts', fontsize=12)\n",
    "plt.ylabel('Number of Unique Clients', fontsize=12)\n",
    "plt.xticks(np.arange(unique_clients_df['num_accts'].min(), unique_clients_df['num_accts'].max() + 1))\n",
    "\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "# Ensure bins are correctly set for integer counts\n",
    "sns.histplot(unique_clients_df['logons_6_mnth'], bins=np.arange(unique_clients_df['logons_6_mnth'].min(), unique_clients_df['logons_6_mnth'].max() + 2) - 0.5, kde=False, color='gold')\n",
    "plt.title('Logons in Last 6 Months Distribution (Experiment Participants)', fontsize=14)\n",
    "plt.xlabel('Number of Logons', fontsize=12)\n",
    "plt.ylabel('Number of Unique Clients', fontsize=12)\n",
    "plt.xticks(np.arange(unique_clients_df['logons_6_mnth'].min(), unique_clients_df['logons_6_mnth'].max() + 1))\n",
    "\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "# Ensure bins are correctly set for integer counts\n",
    "sns.histplot(unique_clients_df['calls_6_mnth'], bins=np.arange(unique_clients_df['calls_6_mnth'].min(), unique_clients_df['calls_6_mnth'].max() + 2) - 0.5, kde=False, color='orchid')\n",
    "plt.title('Calls in Last 6 Months Distribution (Experiment Participants)', fontsize=14)\n",
    "plt.xlabel('Number of Calls', fontsize=12)\n",
    "plt.ylabel('Number of Unique Clients', fontsize=12)\n",
    "plt.xticks(np.arange(unique_clients_df['calls_6_mnth'].min(), unique_clients_df['calls_6_mnth'].max() + 1))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.suptitle('Demographic Overview of Vanguard A/B Test Participants', fontsize=16, y=0.98)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDescriptive statistics for key demographics (Unique Clients in Experiment):\")\n",
    "print(unique_clients_df[['clnt_age', 'clnt_tenure_yr', 'num_accts', 'bal', 'calls_6_mnth', 'logons_6_mnth']].describe())\n",
    "print(\"\\nMode of Gender:\", unique_clients_df['gendr'].mode()[0])\n",
    "\n",
    "\n",
    "# 2. Are the primary clients younger or older, new or long-standing?\n",
    "print(\"\\n-- Age and Tenure Insights (Experiment Participants) --\")\n",
    "avg_age = unique_clients_df['clnt_age'].mean()\n",
    "median_age = unique_clients_df['clnt_age'].median()\n",
    "avg_tenure_yr = unique_clients_df['clnt_tenure_yr'].mean()\n",
    "median_tenure_yr = unique_clients_df['clnt_tenure_yr'].median()\n",
    "\n",
    "print(f\"Average client age: {avg_age:.2f} years, Median age: {median_age:.2f} years\")\n",
    "print(f\"Average client tenure: {avg_tenure_yr:.2f} years, Median tenure: {median_tenure_yr:.2f} years\")\n",
    "\n",
    "if median_age < 35:\n",
    "    age_desc = \"relatively young\"\n",
    "elif median_age >=35 and median_age < 55:\n",
    "    age_desc = \"middle-aged\"\n",
    "else:\n",
    "    age_desc = \"older\"\n",
    "\n",
    "if median_tenure_yr < 5:\n",
    "    tenure_desc = \"newer\"\n",
    "elif median_tenure_yr >= 5 and median_tenure_yr < 15:\n",
    "    tenure_desc = \"established\"\n",
    "else:\n",
    "    tenure_desc = \"long-standing\"\n",
    "\n",
    "print(f\"Interpretation: Primary clients in the experiment are typically {age_desc} and {tenure_desc}.\")\n",
    "\n",
    "\n",
    "# 3. Client behavior analysis (additional relevant questions)\n",
    "print(\"\\n-- Additional Client Behavior Insights (Experiment Participants) --\")\n",
    "\n",
    "# How does logon frequency vary by age and gender?\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x='gendr', y='logons_6_mnth', data=unique_clients_df, palette='pastel')\n",
    "plt.title('Logons by Gender (Experiment Participants)', fontsize=14)\n",
    "plt.xlabel('Gender', fontsize=12)\n",
    "plt.ylabel('Logons in Last 6 Months', fontsize=12)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x='clnt_age', y='logons_6_mnth', data=unique_clients_df, alpha=0.6, hue='gendr', palette='deep')\n",
    "plt.title('Logons vs. Age by Gender (Experiment Participants)', fontsize=14)\n",
    "plt.xlabel('Age', fontsize=12)\n",
    "plt.ylabel('Logons in Last 6 Months', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# How does balance vary by tenure and number of accounts?\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x='clnt_tenure_yr', y='bal', data=unique_clients_df, alpha=0.6, hue='num_accts', palette='magma', size='num_accts', sizes=(20, 400))\n",
    "plt.title('Balance vs. Tenure by Number of Accounts (Experiment Participants)', fontsize=14)\n",
    "plt.xlabel('Years with Vanguard', fontsize=12)\n",
    "plt.ylabel('Balance', fontsize=12)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='num_accts', y='bal', data=unique_clients_df, palette='coolwarm')\n",
    "plt.title('Balance by Number of Accounts (Experiment Participants)', fontsize=14)\n",
    "plt.xlabel('Number of Accounts', fontsize=12)\n",
    "plt.ylabel('Balance', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of process steps in general (before A/B split analysis)\n",
    "print(\"\\nDistribution of Process Steps (Overall in Experiment):\")\n",
    "print(df_full_ab_test['process_step'].value_counts(normalize=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
